{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found scene \"british_museum\"\" at data/image-matching-challenge-2022/train/british_museum\n",
      "Found scene \"sacre_coeur\"\" at data/image-matching-challenge-2022/train/sacre_coeur\n",
      "Found scene \"brandenburg_gate\"\" at data/image-matching-challenge-2022/train/brandenburg_gate\n",
      "Found scene \"piazza_san_marco\"\" at data/image-matching-challenge-2022/train/piazza_san_marco\n",
      "Found scene \"buckingham_palace\"\" at data/image-matching-challenge-2022/train/buckingham_palace\n",
      "Found scene \"colosseum_exterior\"\" at data/image-matching-challenge-2022/train/colosseum_exterior\n",
      "Found scene \"pantheon_exterior\"\" at data/image-matching-challenge-2022/train/pantheon_exterior\n",
      "Found scene \"st_peters_square\"\" at data/image-matching-challenge-2022/train/st_peters_square\n",
      "Found scene \"sagrada_familia\"\" at data/image-matching-challenge-2022/train/sagrada_familia\n",
      "Found scene \"notre_dame_front_facade\"\" at data/image-matching-challenge-2022/train/notre_dame_front_facade\n",
      "Found scene \"st_pauls_cathedral\"\" at data/image-matching-challenge-2022/train/st_pauls_cathedral\n",
      "Found scene \"trevi_fountain\"\" at data/image-matching-challenge-2022/train/trevi_fountain\n",
      "Found scene \"grand_place_brussels\"\" at data/image-matching-challenge-2022/train/grand_place_brussels\n",
      "Found scene \"temple_nara_japan\"\" at data/image-matching-challenge-2022/train/temple_nara_japan\n",
      "Found scene \"lincoln_memorial_statue\"\" at data/image-matching-challenge-2022/train/lincoln_memorial_statue\n",
      "Found scene \"taj_mahal\"\" at data/image-matching-challenge-2022/train/taj_mahal\n",
      "Loaded 350 images.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "src = 'data/image-matching-challenge-2022/train'\n",
    "\n",
    "val_scenes = []\n",
    "for f in os.scandir(src):\n",
    "    if f.is_dir():\n",
    "        cur_scene = os.path.split(f)[-1]\n",
    "        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n",
    "        val_scenes += [cur_scene]\n",
    "\n",
    "selected_scenes = [\"brandenburg_gate\",\"grand_place_brussels\",\"sagrada_familia\"]\n",
    "\n",
    "# TODO\n",
    "scene = \"brandenburg_gate\"\n",
    "\n",
    "images_dict = {}\n",
    "for filename in glob(f'{src}/{scene}/images/*.jpg'):\n",
    "    cur_id = os.path.basename(os.path.splitext(filename)[0])\n",
    "    # OpenCV expects BGR, but the images are encoded in standard RGB\n",
    "    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "print(f'Loaded {len(images_dict)} images.')\n",
    "\n",
    "def read_covisibility_data(filepath):\n",
    "    \"\"\"Read covisibility data from a CSV file and return a dictionary.\"\"\"\n",
    "    covisibility = {}\n",
    "    with open(filepath, mode='r') as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader)  # Skip the header\n",
    "        covisibility = {row[0]: float(row[1]) for row in reader}\n",
    "    return covisibility\n",
    "\n",
    "# Load covisibility data\n",
    "covisibility_dict = read_covisibility_data(f'{src}/{scene}/pair_covisibility.csv')\n",
    "\n",
    "pairs = [key for key, value in covisibility_dict.items()]\n",
    "# Subset classification based on covisibility scores\n",
    "easy_subset = [key for key, value in covisibility_dict.items() if value >= 0.7]\n",
    "difficult_subset = [key for key, value in covisibility_dict.items() if 0.1 <= value < 0.2]\n",
    "bad_subset = [key for key, value in covisibility_dict.items() if value < 0.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with fundamental matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ground truth data for 350 images.\n",
      "\n",
      "Scaling factors: {'british_museum': 2.517, 'brandenburg_gate': 7.38, 'buckingham_palace': 18.75, 'colosseum_exterior': 36.99, 'grand_place_brussels': 10.26, 'lincoln_memorial_statue': 1.85, 'notre_dame_front_facade': 1.36, 'pantheon_exterior': 5.41, 'piazza_san_marco': 7.92, 'sacre_coeur': 20.27, 'sagrada_familia': 4.2, 'st_pauls_cathedral': 7.01, 'st_peters_square': 21.48, 'taj_mahal': 20.76, 'temple_nara_japan': 7.79, 'trevi_fountain': 3.67}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth\n",
    "\n",
    "# Named tuple to store camera intrinsics and extrinsics\n",
    "CameraCalibration = namedtuple('CameraCalibration', ['K', 'R', 'T'])\n",
    "\n",
    "def load_calibration(filepath):\n",
    "    \"\"\"Load calibration data from a CSV file.\"\"\"\n",
    "    calibration_data = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            camera_id = row[0]\n",
    "            K = np.array(list(map(float, row[1].split()))).reshape(3, 3)\n",
    "            R = np.array(list(map(float, row[2].split()))).reshape(3, 3)\n",
    "            T = np.array(list(map(float, row[3].split())))\n",
    "            calibration_data[camera_id] = CameraCalibration(K=K, R=R, T=T)\n",
    "    return calibration_data\n",
    "\n",
    "def load_scaling_factors(filepath):\n",
    "    \"\"\"Load scaling factors from a CSV file.\"\"\"\n",
    "    scaling_factors = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            scaling_factors[row[0]] = float(row[1])\n",
    "    return scaling_factors\n",
    "\n",
    "# Load calibration data and scaling factors\n",
    "calib_dict = load_calibration(f'{src}/{scene}/calibration.csv')\n",
    "print(f'Loaded ground truth data for {len(calib_dict)} images.\\n')\n",
    "\n",
    "scaling_dict = load_scaling_factors(f'{src}/scaling_factors.csv')\n",
    "print(f'Scaling factors: {scaling_dict}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints, K):\n",
    "    \"\"\"Normalize keypoints using the camera intrinsics.\"\"\"\n",
    "    cx, cy = K[0, 2], K[1, 2]\n",
    "    fx, fy = K[0, 0], K[1, 1]\n",
    "    return (keypoints - np.array([[cx, cy]])) / np.array([[fx, fy]])\n",
    "\n",
    "def compute_essential_matrix(F, K1, K2, kp1, kp2):\n",
    "    \"\"\"Compute the Essential matrix from the Fundamental matrix and keypoints.\"\"\"\n",
    "    E = K2.T @ F @ K1\n",
    "    kp1_normalized = normalize_keypoints(kp1, K1)\n",
    "    kp2_normalized = normalize_keypoints(kp2, K2)\n",
    "    _, R, T, _ = cv2.recoverPose(E, kp1_normalized, kp2_normalized)\n",
    "    return E, R, T\n",
    "\n",
    "\n",
    "def quaternion_from_matrix(matrix):\n",
    "    \"\"\"Convert a rotation matrix into a quaternion.\"\"\"\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    trace = np.trace(M)\n",
    "    if trace > 0:\n",
    "        s = np.sqrt(trace + 1.0) * 2\n",
    "        qw = 0.25 * s\n",
    "        qx = (M[2, 1] - M[1, 2]) / s\n",
    "        qy = (M[0, 2] - M[2, 0]) / s\n",
    "        qz = (M[1, 0] - M[0, 1]) / s\n",
    "    else:\n",
    "        idx = np.argmax(np.diagonal(M))\n",
    "        if idx == 0:\n",
    "            s = np.sqrt(1.0 + M[0, 0] - M[1, 1] - M[2, 2]) * 2\n",
    "            qw = (M[2, 1] - M[1, 2]) / s\n",
    "            qx = 0.25 * s\n",
    "            qy = (M[0, 1] + M[1, 0]) / s\n",
    "            qz = (M[0, 2] + M[2, 0]) / s\n",
    "        elif idx == 1:\n",
    "            s = np.sqrt(1.0 + M[1, 1] - M[0, 0] - M[2, 2]) * 2\n",
    "            qw = (M[0, 2] - M[2, 0]) / s\n",
    "            qx = (M[0, 1] + M[1, 0]) / s\n",
    "            qy = 0.25 * s\n",
    "            qz = (M[1, 2] + M[2, 1]) / s\n",
    "        else:\n",
    "            s = np.sqrt(1.0 + M[2, 2] - M[0, 0] - M[1, 1]) * 2\n",
    "            qw = (M[1, 0] - M[0, 1]) / s\n",
    "            qx = (M[0, 2] + M[2, 0]) / s\n",
    "            qy = (M[1, 2] + M[2, 1]) / s\n",
    "            qz = 0.25 * s\n",
    "    return np.array([qw, qx, qy, qz])\n",
    "\n",
    "\n",
    "def compute_error(q_gt, T_gt, q, T, scale):\n",
    "    \"\"\"Compute the error metrics for rotation and translation.\"\"\"\n",
    "    eps = 1e-15\n",
    "    q_gt_normalized = q_gt / (np.linalg.norm(q_gt) + eps)\n",
    "    q_normalized = q / (np.linalg.norm(q) + eps)\n",
    "    loss_q = max(eps, 1.0 - np.sum(q_gt_normalized * q_normalized)**2)\n",
    "    err_q = np.arccos(1 - 2 * loss_q)\n",
    "\n",
    "    T_gt_scaled = T_gt * scale\n",
    "    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n",
    "    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n",
    "\n",
    "    return np.degrees(err_q), err_t\n",
    "\n",
    "# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. The solution is clearly much cleaner, even though it may still contain outliers.\n",
    "def evaluate_one_pair_from_torch(matched_kpts1_coords, matched_kpts2_coords, pair_id):\n",
    "    image_id1, image_id2 = pair_id.split('-')\n",
    "\n",
    "    F, inlier_mask = cv2.findFundamentalMat(\n",
    "        matched_kpts1_coords, \n",
    "        matched_kpts2_coords, \n",
    "        method=cv2.USAC_MAGSAC, \n",
    "        ransacReprojThreshold=0.25, \n",
    "        confidence=0.99999, \n",
    "        maxIters=10000)\n",
    "\n",
    "    inlier_kp_1 = matched_kpts1_coords[inlier_mask.ravel() == 1]\n",
    "    inlier_kp_2 = matched_kpts1_coords[inlier_mask.ravel() == 1]\n",
    "\n",
    "    # Compute the Essential matrix, rotation, and translation\n",
    "    E, R, T = compute_essential_matrix(F, calib_dict[image_id1].K, calib_dict[image_id2].K, inlier_kp_1, inlier_kp_2)\n",
    "    q = quaternion_from_matrix(R)\n",
    "    T = T.flatten()\n",
    "\n",
    "    # Compute ground truth pose differences\n",
    "    R1_gt, T1_gt = calib_dict[image_id1].R, calib_dict[image_id1].T.reshape((3, 1))\n",
    "    R2_gt, T2_gt = calib_dict[image_id2].R, calib_dict[image_id2].T.reshape((3, 1))\n",
    "    dR_gt = R2_gt @ R1_gt.T\n",
    "    dT_gt = (T2_gt - dR_gt @ T1_gt).flatten()\n",
    "    q_gt = quaternion_from_matrix(dR_gt)\n",
    "\n",
    "        # Compute errors\n",
    "    err_q, err_t = compute_error(q_gt, dT_gt, q, T, scaling_dict[scene])\n",
    "    return err_q, err_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90920828_5082887495-20133057_3035445116\n",
      "Successfully loaded output_1_result.torch\n",
      "Pair \"90920828_5082887495-20133057_3035445116\", rotation error: 1.47° | translation error: 2.02 m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/826225559.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_torch = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "def load_torch_file(file_path):\n",
    "    try:\n",
    "        loaded_torch = torch.load(file_path)\n",
    "        \n",
    "        kpts1 = loaded_torch['all_kpts0']\n",
    "        kpts2 = loaded_torch['all_kpts1']\n",
    "        matched_kpts1_coords = loaded_torch['matched_kpts0']\n",
    "        matched_kpts2_coords = loaded_torch['matched_kpts1']\n",
    "        inlier_kpts1 = loaded_torch['inlier_kpts0']\n",
    "        inlier_kpts2 = loaded_torch['inlier_kpts1']\n",
    "        print(f\"Successfully loaded {file_path}\")\n",
    "        return kpts1, kpts2, matched_kpts1_coords, matched_kpts2_coords, inlier_kpts1, inlier_kpts2\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading .torch file: {e}\")\n",
    "        return None\n",
    "\n",
    "#TODO change to pair ID\n",
    "pair_id = pairs[0] # 90920828_5082887495-20133057_3035445116\n",
    "file_path = 'output_1_result.torch'  # Replace with your .torch file path\n",
    "kpts1, kpts2, matched_kpts1_coords, matched_kpts2_coords, inlier_kpts1, inlier_kpts2 = load_torch_file(file_path)\n",
    "# print(\"kpts1\", kpts1.shape, kpts1[0])\n",
    "# print(\"matched_kpts1\",matched_kpts1_coords.shape, matched_kpts1_coords[0])\n",
    "# print(\"matched_kpts2\",matched_kpts2_coords.shape, matched_kpts2_coords[0])\n",
    "# print(\"inlier_kpts1\",inlier_kpts1.shape, inlier_kpts1[0])\n",
    "\n",
    "err_q, err_t = evaluate_one_pair_from_torch(matched_kpts1_coords, matched_kpts2_coords, pair_id)\n",
    "print(f'Pair \"{pair_id}\", rotation error: {err_q:.2f}° | translation error: {err_t:.2f} m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
